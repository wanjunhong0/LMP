{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from utils import *\n",
    "from freebase import *\n",
    "from propagation import *\n",
    "import random\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str,\n",
    "                    default=\"cwq\", help=\"choose the dataset from {webqsp, cwq}.\")\n",
    "parser.add_argument(\"--max_length\", type=int,\n",
    "                    default=1024, help=\"the max length of LLMs output.\")\n",
    "parser.add_argument(\"--limit\", type=int,\n",
    "                    default=7000, help=\"the max length of the approximation LLMs input.\")      \n",
    "parser.add_argument(\"--temperature\", type=float,\n",
    "                    default=0., help=\"the temperature\")\n",
    "parser.add_argument(\"--llm\", type=str,\n",
    "                    default=\"llama-3\", help=\"choose base LLM model from {llama, gpt-3.5-turbo, gpt-4}.\")\n",
    "parser.add_argument(\"--openai_api_key\", type=str,\n",
    "                    default=\"\", help=\"if the LLM is gpt-3.5-turbo or gpt-4, you need add your own openai api key.\")\n",
    "parser.add_argument('--verbose', action='store_true', help=\"print LLM input and output.\")\n",
    "args = parser.parse_args([\"--verbose\"])\n",
    "# args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "datas, question_string = prepare_dataset(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datas[2943]\n",
    "question = data[question_string]\n",
    "topics = data['topic_entity']\n",
    "paths = {topics[topic]: {} for topic in topics}\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topic in topics:\n",
    "for topic in topics:\n",
    "    topic_name = topics[topic]\n",
    "    # 1-hop propagation\n",
    "    relations = get_relations(question, topic, topic_name, args, 3)\n",
    "    entities = get_entities({topic: topic_name}, relations, topic)\n",
    "    [paths[topic_name].update({r: {\"entities\": entities[i]}}) for i, r in enumerate(relations)]\n",
    "    facts = propagate(question, topic_name, relations, paths[topic_name], 1, args)\n",
    "    [paths[topic_name][r].update({\"fact\": facts[i]}) for i, r in enumerate(relations)]\n",
    "    # 2-hop propagation\n",
    "    relations = get_relations_distant(question, topic, topic_name, relations, paths[topic_name], args, 3)\n",
    "    entities = get_entities_distant(paths[topic_name], relations, topic)\n",
    "    [paths[topic_name].update({r: {\"entities\": entities[i]}}) for i, r in enumerate(relations)]\n",
    "    facts = propagate(question, topic_name, relations, paths[topic_name], 2, args)\n",
    "    [paths[topic_name][r].update({\"fact\": facts[i]}) for i, r in enumerate(relations)]\n",
    "    # 3-hop propagation\n",
    "    relations = get_relations_distant(question, topic, topic_name, relations, paths[topic_name], args, 3)\n",
    "    entities = get_entities_distant(paths[topic_name], relations, topic)\n",
    "    [paths[topic_name].update({r: {\"entities\": entities[i]}}) for i, r in enumerate(relations)]\n",
    "    facts = propagate(question, topic_name, relations, paths[topic_name], 3, args)\n",
    "    [paths[topic_name][r].update({\"fact\": facts[i]}) for i, r in enumerate(relations)]\n",
    "    # # # # clean paths\n",
    "    [paths[topic_name].update({r: paths[topic_name][r]['fact']}) for r in paths[topic_name]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = construct_facts(paths, topics)\n",
    "prompt = question_prompt.format(facts, question)\n",
    "response = run_llm(prompt, args)\n",
    "output = {\"question\": question, \"result\": response, \"paths\": paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_2_jsonl(\"lmp_{}_{}_3hop.jsonl\".format(args.dataset, args.llm), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql_name = \"\"\"\n",
    "PREFIX ns: <http://rdf.freebase.com/ns/>\n",
    "SELECT DISTINCT ?e ?name\n",
    "WHERE {\n",
    "?e ns:type.object.name ?name . \n",
    "FILTER(CONTAINS(LCASE(?name), \"National Football League\"))\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "execute_sparql(sparql_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_r = \"\"\"\n",
    "PREFIX ns: <http://rdf.freebase.com/ns/>\n",
    "SELECT DISTINCT ?e ?r ?name\n",
    "WHERE {\n",
    "?e ?r ns:%s.\n",
    "?e ns:type.object.name ?name . \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "execute_sparql(sparql_r % ('m.0cs1bx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'m.03m5x4': 'The NBA Finals'},\n",
    "{\"m.059yj\": \"National Football League\"},\n",
    "{'m.06x5s': 'Super Bowl'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sparql(sparql_relations % 'm.076ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sparql(sparql_entities % ('ns:m.04thp ns:m.0j4b', 'location.statistical_region.part_time_employment_percent', 'm.04thp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_distant(paths, relations, topic):\n",
    "    entities = []\n",
    "    for relation in relations:\n",
    "        start_entities = {}\n",
    "        previous_entities = paths[relation.rsplit('->', 1)[0]]['entities']\n",
    "        for i in previous_entities:\n",
    "            for j in previous_entities[i]:\n",
    "                if j not in ['literal', 'typed-literal']:\n",
    "                    start_entities.update({j: previous_entities[i][j]})\n",
    "        print(start_entities)\n",
    "        sparql_output = execute_sparql(sparql_entities % (' '.join(['ns:' + i for i in list(start_entities.keys())]), relation.rsplit('->', 1)[1], topic))\n",
    "        filtered_entities = filter_entities(start_entities, sparql_output)\n",
    "\n",
    "        entities.append(filtered_entities)\n",
    "    \n",
    "    return entities, sparql_output\n",
    "\n",
    "\n",
    "get_entities_distant(paths[topic_name], [relations[0]], topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sparql(sparql_relations_3hop % ('m.06x5s', 'sports.sports_championship.events', 'sports.sports_championship_event.season', 'm.06x5s'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
